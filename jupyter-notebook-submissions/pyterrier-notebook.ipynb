{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTerrier Notebook for Full-Rank Submissions\n",
    "\n",
    "This notebook serves as a baseline full-rank submission for [TIRA](https://tira.io)/[TIREx](https://tira.io/tirex) that builds a PyTerrier index and subsequently creates a run with BM25."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Ensure Libraries are Imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "import re\n",
    "\n",
    "# Detect if we are in the TIRA sandbox\n",
    "# Install the required dependencies if we are not in the sandbox.\n",
    "if 'TIRA_DATASET_ID' not in os.environ:\n",
    "    !pip3 install python-terrier tira==0.0.88 ir_datasets #nltk spacy\n",
    "    #!python -m spacy download en_core_web_sm\n",
    "else:\n",
    "    print('We are in the TIRA sandbox.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tira.third_party_integrations import ensure_pyterrier_is_loaded, persist_and_normalize_run\n",
    "\n",
    "# this loads and starts pyterrier so that it also works in the TIRA\n",
    "ensure_pyterrier_is_loaded()\n",
    "\n",
    "# PyTerrier must be imported after the call to ensure_pyterrier_is_loaded in TIRA.\n",
    "import pyterrier as pt\n",
    "\n",
    "if not pt.started():\n",
    "    pt.init(boot_packages=['mam10eks:custom-terrier-token-processing:0.0.1', 'com.github.terrierteam:terrier-prf:-SNAPSHOT'])\n",
    "    from jnius import autoclass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.1: Load Stopword-List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# generate custom stopword list\n",
    "nltk.download('stopwords')\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "spacy_stopwords = set(nlp.Defaults.stop_words)\n",
    "sklearn_stopwords = set(ENGLISH_STOP_WORDS)\n",
    "combined_stopwords = set.union(nltk_stopwords, spacy_stopwords, sklearn_stopwords)\n",
    "\n",
    "!rm -Rf /tmp/index\n",
    "file_path = \"custom_stopwords.txt\"\n",
    "\n",
    "with open(file_path, 'w+') as file:\n",
    "    for element in combined_stopwords:\n",
    "        file.write(element+ \"\\n\")\n",
    "\n",
    "pt.set_property('stopwords.filename','./custom_stopwords.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pt.get_dataset('irds:ir-lab-jena-leipzig-wise-2023/validation-20231104-training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source of this cell and the realted abbr. list: https://github.com/ipekdk/abbreviation-list-english\n",
    "# Source code has been altered.\n",
    "abbr_path=None\n",
    "if 'TIRA_DATASET_ID' not in os.environ:\n",
    "    abbr_path = './abbreviations_eng.xls'\n",
    "else:\n",
    "    # use abs path on tira\n",
    "    abbr_path = '/workspace/abbreviations_eng.xls'\n",
    "abbr=pd.read_excel(abbr_path)\n",
    "old = abbr['abbr']\n",
    "new = abbr['long']\n",
    "\n",
    "state_machines = []\n",
    "for i in range(0,len(abbr)):\n",
    "    state_machines.append(re.compile(f\"({re.escape(str(old[i]))})\"))\n",
    "\n",
    "# Fix invalid characters\n",
    "for i in range(0,len(new)):\n",
    "    new[i] = new[i].replace('/', '')\n",
    "\n",
    "def abbr_expansion(text):\n",
    "    for i in range(0,len(abbr)):         \n",
    "        text = re.sub(state_machines[i], str(new[i]), text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_weight_function(value, max_length):\n",
    "    \"\"\"Linear weight function, which applies linearly decreasing weights, such taht the last term has weight 1.\"\"\"\n",
    "    return -value+max_length\n",
    "\n",
    "def centered_parabola_weight_function(value, max_length):\n",
    "    \"\"\"Centered parabola weight function, which applies decreasing and increasing weights, such taht the middle term has weight 1.\"\"\"\n",
    "    multiplier = 0.5\n",
    "    return (value*multiplier - (max_length-1)*multiplier*0.5)**2 + 1\n",
    "\n",
    "def log_weight_function(value, max_length):\n",
    "    \"\"\"Logarithmic weight function, which applies decreasing weights, such taht the last term has weight 1.\"\"\"\n",
    "    return - math.log2((value+1)/(value+max_length+1))\n",
    "\n",
    "def apply_query_term_weighing(query, weight_function):\n",
    "    query_parts = query.split()\n",
    "    query_length = len(query_parts)\n",
    "    weights = [weight_function(x, query_length) for x in range(query_length)]\n",
    "\n",
    "    return \" \".join([f\"{query_part}^{weight}\" for query_part, weight in zip(query_parts,weights)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, is_query = False):\n",
    "    # Only apply abbr expansion to queries\n",
    "    if is_query:\n",
    "        text = abbr_expansion(text)\n",
    "        text = apply_query_term_weighing(text, linear_weight_function)\n",
    "    return text\n",
    "\n",
    "def preprocess_corpus(data):\n",
    "    processed_corpus = []\n",
    "    for element in data.get_corpus_iter():\n",
    "        element['text'] = preprocess_text(element['text'])\n",
    "        processed_corpus.append(element)\n",
    "\n",
    "    return processed_corpus\n",
    "\n",
    "processed_corpus = preprocess_corpus(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = data.get_topics('title')\n",
    "for entry in topics.iterrows():\n",
    "    query = entry[1][\"query\"]\n",
    "    query = preprocess_text(query, is_query=True)\n",
    "    entry[1][\"query\"] = query\n",
    "print('See the first two queries:')\n",
    "print(topics.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Build the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Build index:')\n",
    "# Both the indexer and batch retrieve use terriers default porter stemmer and a default stopword list (englisch)\n",
    "# TODO: consider adding french stopwords\n",
    "iter_indexer = pt.IterDictIndexer(\"/tmp/index\", overwrite = True, blocks = True,meta = {'docno':100, 'text': 20480}, stemmer = 'PorterStemmer')\n",
    "!rm -Rf /tmp/index\n",
    "index_ref = iter_indexer.index(processed_corpus)\n",
    "\n",
    "print('Done. Index is created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create the Retrieval Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pt.IndexFactory.of(index_ref)\n",
    "\n",
    "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\", verbose=True)\n",
    "pl2 = pt.BatchRetrieve(index, wmodel=\"PL2\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.1: Add Query Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Expansion\n",
    "bo1 = pt.rewrite.Bo1QueryExpansion(index) \n",
    "\n",
    "#Pipeline\n",
    "pipe = (bm25 % 100) >> bo1 >> pl2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Create the Run and Persist the Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Create run')\n",
    "\n",
    "run = pipe(topics)\n",
    "\n",
    "print('Done, run was created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doesn't work in TIRA, only for local testing\n",
    "#pt.Experiment(\n",
    "#    [bm25, pipe],\n",
    "#    data.get_topics()[:50],\n",
    "#    data.get_qrels(),\n",
    "#    eval_metrics=[\"map\", \"recip_rank\", \"P_10\", \"recall_10\", \"ndcg\"],\n",
    "#    names=[\"BM25\", \"Spotted Turtle\"],\n",
    "#    baseline=0\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_and_normalize_run(run, 'bm25-custom-stopwords')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
