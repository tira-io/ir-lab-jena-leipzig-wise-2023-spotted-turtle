{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTerrier Notebook for Full-Rank Submissions\n",
    "\n",
    "This notebook serves as a baseline full-rank submission for [TIRA](https://tira.io)/[TIREx](https://tira.io/tirex) that builds a PyTerrier index and subsequently creates a run with BM25."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Ensure Libraries are Imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "# Detect if we are in the TIRA sandbox\n",
    "# Install the required dependencies if we are not in the sandbox.\n",
    "if 'TIRA_DATASET_ID' not in os.environ:\n",
    "    !pip3 install python-terrier tira==0.0.88 ir_datasets\n",
    "else:\n",
    "    print('We are in the TIRA sandbox.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tira.third_party_integrations import ensure_pyterrier_is_loaded, persist_and_normalize_run\n",
    "\n",
    "# this loads and starts pyterrier so that it also works in the TIRA\n",
    "ensure_pyterrier_is_loaded()\n",
    "\n",
    "# PyTerrier must be imported after the call to ensure_pyterrier_is_loaded in TIRA.\n",
    "import pyterrier as pt\n",
    "from pyterrier.measures import *\n",
    "\n",
    "if not pt.started():\n",
    "    pt.init(boot_packages=['mam10eks:custom-terrier-token-processing:0.0.1', 'com.github.terrierteam:terrier-prf:-SNAPSHOT'])\n",
    "    from jnius import autoclass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pt.get_dataset('irds:ir-lab-jena-leipzig-wise-2023/validation-20231104-training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Build the Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create the Retrieval Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.1: Add Query Expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Create the Run and Persist the Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tira import Client\n",
    "tira_client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "\n",
    "if 'TIRA_DATASET_ID' not in os.environ:\n",
    "    # Führe das Experiment durch\n",
    "    # PyTerrier führt standardmäßig nur two-sided ttest durch, daher müssen wir hier unseren eigenen one-sided ttest konfigurieren\n",
    "    # PerQuery gibt für jede Query die eval_metrics zurück\n",
    "    experiment = pt.Experiment(\n",
    "        [\n",
    "            # TODO: Load runs from TIRA\n",
    "            pt.Transformer.from_df(\n",
    "                tira_client.get_run_output(\n",
    "                    '<task>/<team>/<approach>', '<dataset>')),\n",
    "            pt.Transformer.from_df(\n",
    "                tira_client.get_run_output(\n",
    "                    '<task>/<team>/<approach>', '<dataset>')),\n",
    "            pt.Transformer.from_df(\n",
    "                tira_client.get_run_output(\n",
    "                    '<task>/<team>/<approach>', '<dataset>')),\n",
    "        ],\n",
    "        data.get_topics(),\n",
    "        data.get_qrels(),\n",
    "        eval_metrics=[\"ndcg_cut_5\"],\n",
    "        names=[\n",
    "            \"BM25\",\n",
    "            \"BM25 with linear weighting (ascending weight)\",\n",
    "            \"BM25 with linear weighting (descending weight)\",\n",
    "        ],\n",
    "        perquery=True\n",
    "    )\n",
    "    \n",
    "    # Speicher alle Werte einer Pipeline in einem DF\n",
    "    linear = experiment[experiment.name == \"BM25 with linear weighting (ascending weight)\"]\n",
    "    inv_linear = experiment[experiment.name == \"BM25 with linear weighting (descending weight)\"]\n",
    "    \n",
    "    # Es werden nur die nDCG-Werte gebraucht\n",
    "    inv_linear = inv_linear[\"value\"]\n",
    "    linear = linear[\"value\"]\n",
    "    \n",
    "    # Das ist die unterliegende Test-Funktion, die auch von PyTerrier verwendet wird. Statt \"two-sided\" wird hier \"greater\" verwendet.\n",
    "    print(ttest_rel(linear, inv_linear, alternative=\"greater\").pvalue)\n",
    "\n",
    "    # TODO: Ggf. Vergleich mit BM25 ohne weighting?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
